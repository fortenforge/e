---
layout: post
title:  "Survey of Discrete Log Algorithms"
date:   2017-12-01 3:12:44 -0500
categories: crypto
use_math: true
---

In 1999 Dan Boneh published a survey paper called ["Twenty years of attacks on the RSA cryptosystem"](http://crypto.stanford.edu/~dabo/pubs/papers/RSA-survey.pdf). I like to tell people that if they read the entire paper they can solve just about every CTF challenge involving RSA.

This post is my attempt to do something similar with the discrete log problem, a primitive that underlies a bunch of cryptographic protocols including the Diffie-Hellman key exchange and the ElGamal encryption system. The algorithms all have similar-sounding names, and I wanted to just sit down and describe each of them to get them straight in my head.

<!--more-->

## The Discrete Log Problem

Suppose we have a group \\(G\\) generated by an element \\(g\\) (this means that every element can be written as \\(g^k\\) for some integer \\(k\\).) The
*discrete log* of an element \\(y = g^x\\) is the number \\(x\\). For cyclic groups of order \\(n\\), discrete logs are unique modulo \\(n\\).

A common choice for \\(G\\) in cryptography is \\((\mathbb{Z}_p)^\times\\), or the multiplicative group modulo the prime \\(p\\). Fermat's little theorem tells us that \\(g^{p-1} = 1 \pmod{p}\\), so the order of \\((\mathbb{Z}_p)^\times\\) is \\(p-1\\).

If the order of the cyclic group is not a [smooth number](https://en.wikipedia.org/wiki/Smooth_number), i.e. if \\(p-1\\) has at least one large prime factor, then it is believed that it is hard to compute the discrete logarithm on elements of the group.

This hardness assumption forms the basis for a large portion of modern cryptography. As a result, it's important to understand the best algorithms that exist to compute discrete logs.

## Approach #1: Brute Force

Given an element \\(y\\) in a group \\(G\\) with generator \\(g\\), we're trying to find an integer \\(x\\) such that \\(g^x = y\\). A naive way to do this is to simply try all possible \\(x\\). This approach has time complexity \\(O(n)\\) where \\(n\\) is the order of the group and space complexity \\(O(1)\\).

This website is secured using an elliptic curve group with order \\[n = 2^{256} - 2^{224} + 2^{192} - 89188191075325690597107910205041859247.\\] Using this approach to find a discrete log in this group would take around \\(2^{256}\\) trials, which is a little much.

## Approach #2: Baby-step giant-step

There's a relatively easy way to improve the running time of the brute-force approach using a [common technique](https://en.wikipedia.org/wiki/Meet-in-the-middle_attack) in cryptography. We're trying to find \\(x\\) such that \\(g^x = y\\). Let's rewrite \\(x\\) as \\(im + j\\) where \\(m = \lceil n \rceil\\) for integers \\(i\\) and \\(j\\) between \\(0\\) and \\(m\\). Now we can rewrite \\[y = g^x = g^{im + j}\\] as \\[(g^{-m})^i y = g^j.\\]

Next, let's compute and store \\(g^j\\) for all possible values of \\(j\\) in a set, and then run through every possible value of \\(i\\) and check when \\((g^{-m})^i y\\) is in our set. Once we find a colliding \\(i\\) and \\(j\\), we can compute \\(x = im + j\\).

This is an example of a space-time tradeoff; we've essentially turned an \\(O(n)\\) time, \\(O(1)\\) space algorithm into an \\(O(\sqrt{n})\\) time, \\(O(\sqrt{n})\\) space algorithm.

The name stems from the fact that every increment of \\(j\\) represents a "baby-step" of size \\(g^1\\) while every increment of \\(i\\) represents a "giant-step" of size \\(g^m\\).

## Approach #3: Pollard's \\(\rho\\) algorithm

It turns out it's possible to compute discrete logs with \\(O(\sqrt{n})\\) time and \\(O(1)\\) space. Here's the idea:

First, partition the group into three pairwise-disjoint, approximately-equally sized sets \\(S_0, S_1,\\) and \\(S_2\\) such that \\(G = S_0 \cup S_1 \cup S_2 \\). This partition should be made at random.

Consider the following series \\(h_i = g^{a_i}y^{b_i}\\) where \\(a_0 = b_0 = 1\\) and \\(h_i = f(h_{i-1})\\) where

$$f(h) =
\begin{cases}
gh,  & h \in S_0 \\
h^2, & h \in S_1 \\
yh, & h \in S_2
\end{cases}$$

Basically, a third of the time we increase \\(a_i\\) by one, a third of the time we increase \\(b_i\\) by one, and a third of the time we double both \\(a_i\\) and \\(b_i\\). This is just a convenient way to make the series walk through the space of \\(g^{a_i}y^{b_i}\\) pseudorandomly.

Why do we want to walk through this space? Well, consider what happens if we find a collision; that is, four integers \\(a, b, a', b'\\) such that

\\[g^ay^b = g^{a'}y^{b'}.\\]

We can rewrite this as \\[y = g^{(a - a') / (b' - b)}\\] and then compute the discrete log of \\(y\\) as \\((a - a') / (b' - b)\\) (division just means "multiply by the inverse modulo \\(n\\), the order of the group").

So how do we find a collision in our series using \\(O(1)\\) space? A collision in our series means that our series contains a cycle, and there's a [common trick](https://en.wikipedia.org/wiki/Cycle_detection#Floyd's_Tortoise_and_Hare) that frequently pops up in algorithms interviews that allows us to find cycles in linked lists using \\(O(1)\\) space.

The idea is to use two iterators. One iterator increments through the series by one at each step, while the other increments by two at each step. If the linked list has a cycle, the two iterators will, at some point, have the same value.

For our series, we'll use two variables, \\(s\\) and \\(t\\) both initialized to \\(h_0\\). On each step of our algorithm, we'll set \\(s\\) to \\(f(s)\\) and \\(t\\) to \\(f(f(t))\\) and check if the two are equal. If they are, we have a collision and can solve the DLP. If not, continue.

The name "rho" stems from the fact that the series with a cycle, when considered as a directed graph, takes the shape of the Greek letter \\(\rho\\). Pollard also has an analogous algorithm for factorization.

## Approach #4: Pollard's kangaroo algorithm

In the same paper where he introduced his rho-algorithm, Pollard also discussed a different algorithm to solve the discrete log problem.

## Approach #5: The Pohlig-Hellman algorithm

Remember how we said earlier that the order of the group needs to not be a smooth number? Here's what goes wrong if \\(n\\\) has a bunch of small factors:

## Comparison table

<table style="width:100%">
  <tr>
    <th>Algorithm</th>
    <th>Time Complexity</th>
    <th>Space Complexity</th>
    <th>Notes</th>
  </tr>
  <tr>
    <td>Brute Force</td>
    <td>\(O(n)\)</td>
    <td>\(O(1)\)</td>
    <td>Don't use.</td>
  </tr>
  <tr>
    <td>Baby-step Giant-step</td>
    <td>\(O(\sqrt{n})\)</td>
    <td>\(O(\sqrt{n})\)</td>
    <td>Easy to implement, but Pollard's \(\rho\) is better.</td>
  </tr>
  <tr>
    <td>Pollard's \(\rho\)</td>
    <td>\(O(\sqrt{n})\)</td>
    <td>\(O(1)\)</td>
    <td>Fastest known DLP algorithm for prime order.</td>
  </tr>
  <tr>
    <td>Pollard's kangaroo</td>
    <td>\(O(\sqrt{b-a})\)</td>
    <td>\(O(1)\)</td>
    <td>Use when you already know some information about \(x\).</td>
  </tr>
  <tr>
    <td>Polhig-Hellman</td>
    <td>\(O(\sqrt{p})\)</td>
    <td>\(O(\text{# of primes})\)</td>
    <td>Use when \(n\) is factorable into many small primes.</td>
  </tr>
</table>
